%!TEX root = ../main.tex

\section{Methods}

\subsection{Distributed file loading}

Since the size of the final processed questions file is approximately $11$GB,
it will not be feasible to load into memory on most laptops. Therefore it will
be necessary to load the file in smaller chunks.

The following code illustrates how the file \texttt{posts.csv} can be divided
into byte-chunks. I.e. the following generator yields a list of tuples
\texttt{(from\_byte, size)} where \texttt{from\_byte} is the index in the file in
bytes and \texttt{size} is the size of the given chunk in bytes.

\begin{lstlisting}[language=python]
with open('posts.csv', 'rb') as f:
  while True:
    start = f.tell()
    f.seek(chunk_size, 1)
    s = f.readline()
    yield start, f.tell() - start
    if not s:   break
\end{lstlisting}

The \texttt{chunk\_size} is a given minimum size of each chunk. The
\texttt{f.readline()} makes sure the chunk ends at the end of a line.

A chunk of lines from the file can then be loaded using the following lines

\begin{lstlisting}[language=python]
# Seek to chunk start bytes
f.seek(from_bytes)

# Read end of chunk until end of line
chunk = f.read(size)

# Split in lines (Removing the last newline)
lines = chunk.rstrip('\n').split('\n')
\end{lstlisting}


\subsection{Feature hashing}

In order to work with the text data each line of word indices is transformed into
a sparse matrix using the word dictionary and \texttt{scipy}'s
\textit{Compressed Sparse Row matrix} \texttt{scipy.sparse.csr\_matrix}.

A file chunk can be converted to a sparse matrix representation in the following
way (simplified code):

\begin{lstlisting}[language=python]
indptr, indices, data, tags = [0], [], [], []
for input_indices in chunk_to_indices(chunk):
  for idx in input_indices:
    indices.append(idx)
    data.append(1)
  indptr.append(len(indices))

X = csr_matrix(
  (data, indices, indptr),
  shape=(len(indptr) - 1, word_count)
)
\end{lstlisting}

\subsection{K-means clustering}

\subsubsection{Serial}

The regular serial in-memory version of K-means clustering algorithm is shown in
\cref{alg:k-means}.
\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{KMeansClustering}{X, K}
    \State \# Initialize cluster centers
    \For{$k=0$ to $K-1$}
      \State $\mu_k \gets \text{random point in X}$
    \EndFor
    \State \# Run iterations
    \While{$iter < max\_iter$}
      \State \# Update cluster means
      \State $\mu_\text{old} = \mu$
      \For{$k=0$ to $K-1$}
        \State $C_k \gets \{\text{Points in }X\text{ closest to }\mu_k\}$
        \State $\mu_k \gets \frac{1}{\vert C_k \vert} \sum_{x_i \in C_k} x_i$
      \EndFor
      \State \# Check convergence criteria
      \State $norm \gets \left\Vert \mu - \mu_\text{old} \right\Vert$
      \If{$norm < \epsilon$}
        \State break
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{Serial K-means clustering algorithm}
  \label{alg:k-means}
\end{algorithm}

The distance measure used for finding closest points in $X$ is the
\textit{cosine similarity}. I.e. the distance will be defined as
\begin{equation}
  \text{dist}(x_1, x_2) = \frac{x_1 x_2}{\left\Vert x_1 \right\Vert \left\Vert x_2 \right\Vert}
\end{equation}

\subsubsection{Distributed}

The proposed distributed K-means clustering algorithm, which loads the data matrix
$X$ in chunks, is shown in \cref{alg:k-means-dist}.
\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{KMeansClusteringDistributed}{X, K}
    \State \# Initialize cluster centers
    \For{$k=0$ to $K-1$}
      \State $\mu_k \gets \text{random point in X}$
    \EndFor
    \State \# Run iterations
    \While{$iter < max\_iter$}
      \State \# Initialize shared cluster sums and cluster point counts.
      \For{$k=0$ to $K-1$}
        \State $C\text{sum}_k \gets \mathbf{0}$
        \State $C\text{count}_k \gets 0$
      \EndFor

      \State \# Process each chunk in a distributed manner
      \ForAll{Chunks $X_\text{chunk}$ in X}
        \For{$k=0$ to $K-1$}
          \State $C_k \gets \{\text{Points in }X_\text{chunk}\text{ closest to }\mu_k\}$
          \State $C\text{sum}_k \gets C\text{sum}_k + \sum_{x_i \in C_k} x_i$
          \State $C\text{count}_k \gets C\text{count}_k + \vert C_k \vert$
        \EndFor
      \EndFor

      \State \# Gather results and update cluster means
      \State $\mu_\text{old} = \mu$
      \For{$k=0$ to $K-1$}
        \State $\mu_k \gets \frac{C\text{sum}_k}{C\text{count}_k}$
      \EndFor

      \State \# Check convergence criteria
      \State $norm \gets \left\Vert \mu - \mu_\text{old} \right\Vert$
      \If{$norm < \epsilon$}
        \State break
      \EndIf
    \EndWhile
  \end{algorithmic}
  \caption{Distributed K-means clustering algorithm}
  \label{alg:k-means-dist}
\end{algorithm}

\subsubsection{Implementation}

Simplified implementation of a single iteration of the distributed K-means
(see full code in \cref{app:k-means}):

\begin{lstlisting}[language=python]
cluster_sums   = {k: np.zeros((1, word_count)) for k in range(0, K)}
cluster_counts = {k: 0 for k in range(0, K)}

for chunk in chunks:

  # Load chunk lines to sparse matrix
  X = chunk_to_sparse_mat(chunk)

  # Get closest cluster indices
  max_idx = sparse_matrix_to_cluster_indices(X, mu)

  # Assign points to clusters
  mu_subs = collections.defaultdict(list)
  for i, k in enumerate(max_idx):
    mu_subs[k].append(X[i].toarray())

  # Compute sub-means
  for k in range(0, K):
    mu_sub = mu_subs[k]
    if len(mu_sub) == 0:    continue
    cluster_sums[k] += np.asarray(mu_sub).mean(axis=0)
    cluster_counts[k] += 1

# Save old means
mu_old = np.array(mu, copy=True)

# Update means
for k in range(0, K):
  count = cluster_counts[k]
  if count == 0:  continue
  mu[k] = cluster_sums[k] / cluster_counts[k]

# Check convergence criteria
mu_norm = np.linalg.norm(mu - mu_old)

if mu_norm < epsilon:
  print('Converged after %d iterations' % (iteration+1))
  break
\end{lstlisting}

\subsection{Distributed decision tree ensemble}

An alternative to the unsupervised approach is a supervised approach using
an ensemble of decision trees. This idea is inspired by the
\textit{Random Forest} model which is an ensemble of decision trees trained on
bootstrapped features.

For this specific task the decision trees are trained on each chunk of data.
Hence the randomness here lies more in the fact that each tree will only see
a subset of the training data.

The algorithm outline can be seen in \cref{alg:decision-trees}.

\begin{algorithm}[H]
  \begin{algorithmic}[1]
    \Procedure{DecisionTreeEnsembleDistributed}{X, Y, K}

    \State \# Process each chunk in a distributed manner
    \ForAll{Chunks $(X_\text{chunk}, Y_\text{chunk})$ in (X, Y)}
      \State $T \gets \text{Decision tree trained on } (X_\text{chunk}, Y_\text{chunk})$
      \State \# Dump decision tree to file
    \EndFor

  \end{algorithmic}
  \caption{Distributed decision tree ensemble algorithm}
  \label{alg:decision-trees}
\end{algorithm}

Finally the prediction of tags can be done by computing tag probabilities for
each trained decision tree, and basing the prediction on the mean tag probabilities
of all classifiers.

\subsubsection{Implementation}

Simplified implementation of the training of the distributed decision tree
ensemble algorithm (see full code in \cref{app:decision-trees}):

\begin{lstlisting}[language=python]
from sklearn.tree import DecisionTreeClassifier
from sklearn.externals import joblib

for chunk in chunks:

  # Convert to sparse matrix
  X, Y = chunk_to_sparse_mat(chunk)

  # Train decision tree
  clf = DecisionTreeClassifier(
      splitter='best',
      max_features='auto',
      max_depth=None,
  )

  # Fit data
  clf.fit(X, Y)

  # Save trained classifier
  joblib.dump(clf, classifier_filename)
\end{lstlisting}

Here the \textit{scikit-learn} library is used for the implementation of
the decision tree class \texttt{DecisionTreeClassifier}.

\subsection{Parallel processing}

Since these proposed algorithms are implemented in a distributed manner it makes
sense to run them in parallel. For this the Python \texttt{multiprocessing}
library is used.

The general parallel implementations used in this project follow the following
structure:

\begin{lstlisting}[language=python]
import multiprocessing

# Initialize shared variable manager
manager = multiprocessing.Manager()
lock = multiprocessing.Lock()

for chunks in list_of_chunks:
  p = multiprocessing.Process(
    target=process_chunks,
    kwargs={
      'chunks': chunks,
      ...
      'lock': lock
    }
  )
  processes.append(p)

# Start processes
for p in processes:
    p.start()

# Wait for processes to finish
for p in processes:
    p.join()

# Use results
# ...
\end{lstlisting}

where \texttt{process\_chunks} processes a set of chunks of the data file (and
thereby varies from K-means to decision tree ensemble). The \texttt{manager}
is used for sharing values between spawned processes and the \texttt{lock} is
used for making sure multiple processes are not writing to a shared value
simultaneously (which will result in only one of the values being actually
written).

Example of how the multiprocess lock works
\begin{lstlisting}[language=python]
with lock:
  shared_counter += 1
\end{lstlisting}
